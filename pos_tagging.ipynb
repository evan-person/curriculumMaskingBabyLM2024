{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eglucas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagore is trying to learn the language of the babies.\n",
      "['Tagore', 'is', 'trying', 'to', 'learn', 'the', 'language', 'of', 'the', 'babies', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/eglucas/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/home/eglucas/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m word_tokenize(sent)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_tokens)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_tokens\u001b[49m\u001b[43m)\u001b[49m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tag/__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tag/__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/tag/perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[0;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/home/eglucas/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# code for pos tagging \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import random\n",
    "\n",
    "sentence = \"Tagore is trying to learn the language of the babies. But it is very difficult to understand them.\"\n",
    "\n",
    "for sent in sent_tokenize(sentence):\n",
    "    print(sent)\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    print(word_tokens)\n",
    "    print(nltk.pos_tag(word_tokens), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5672c0c1df5c4abd968daeb701a0309c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "866c00e656b74eb9b1a7d84ee438b267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c364b198d24bea9a99112aad2d2723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4fa17168e943c3bccda290bc4cd8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tag', '##ore', 'is', 'trying', 'to', 'learn', 'the', 'language', 'of', 'the', 'babies', '.', 'but', 'it', 'is', 'very', 'difficult', 'to', 'understand', 'them', '.']\n",
      "[6415, 5686, 2003, 2667, 2000, 4553, 1996, 2653, 1997, 1996, 10834, 1012, 2021, 2009, 2003, 2200, 3697, 2000, 3305, 2068, 1012]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kgli/babylm/babylm_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenizing the sentence using bert tokenizer\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(tokenizer.tokenize(sentence))\n",
    "#print token id \n",
    "print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tagore', 'NN'), ('is', 'VBZ'), ('trying', 'VBG'), ('to', 'TO'), ('learn', 'VB'), ('the', 'DT'), ('language', 'NN'), ('of', 'IN'), ('the', 'DT'), ('babies', 'NNS'), ('.', '.'), ('But', 'CC'), ('it', 'PRP'), ('is', 'VBZ'), ('very', 'RB'), ('difficult', 'JJ'), ('to', 'TO'), ('understand', 'VB'), ('them', 'PRP'), ('.', '.')]\n",
      "['tag', '##ore', 'is', 'trying', 'to', 'learn', 'the', 'language', 'of', 'the', 'babies', '.', 'but', 'it', 'is', 'very', 'difficult', 'to', 'understand', 'them', '.']\n",
      "[6415, 5686, 2003, 2667, 2000, 4553, 1996, 2653, 1997, 1996, 10834, 1012, 2021, 2009, 2003, 2200, 3697, 2000, 3305, 2068, 1012]\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "# function to word mask to token mask by using bert tokenizer\n",
    "def mask_sentence(sentence):\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    masked_sentence = []\n",
    "    for token in tokenized_sentence:\n",
    "        masked_sentence.append(token)\n",
    "    return masked_sentence\n",
    "\n",
    "# function to pos tag the sentence by using nltk\n",
    "def pos_tag_sentence(sentence):\n",
    "    # tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    pos_tagged_sentence = nltk.pos_tag(word_tokenize(sentence))\n",
    "    return pos_tagged_sentence\n",
    "\n",
    "# function to get the pos tag of a word\n",
    "def get_pos_tag(word):\n",
    "    return nltk.pos_tag([word])[0][1]\n",
    "\n",
    "# function to print the token ids of the sentence by using bert tokenizer\n",
    "def print_token_ids(sentence):\n",
    "    print(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence)))\n",
    "\n",
    "print(pos_tag_sentence(sentence))\n",
    "print(mask_sentence(sentence))\n",
    "print_token_ids(sentence)\n",
    "print(get_pos_tag('Tagore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sentence: ['[MASK]', '[MASK]', 'is', 'trying', 'to', 'learn', 'the', '[MASK]', 'of', 'the', 'babies', '.', 'but', 'it', 'is', 'very', 'difficult', 'to', 'understand', 'them', '.']\n",
      "Masked Token IDs: [103, 103, 2003, 2667, 2000, 4553, 1996, 103, 1997, 1996, 10834, 1012, 2021, 2009, 2003, 2200, 3697, 2000, 3305, 2068, 1012]\n"
     ]
    }
   ],
   "source": [
    "# code for masking the word and tokens for given pos tag \n",
    "\n",
    "# if i want to mask  with the tag \"NN\" then the sentence should mask the tag \"NN\" word and also the tokens of the tag \"NN\" word  should be masked\n",
    "# also mask the tokens of the tag \"NN\" word\n",
    "\n",
    "def mask_word_and_tokens(sentence, pos_tag):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tagged_sentence = nltk.pos_tag(words)\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "\n",
    "    masked_sentence = []\n",
    "    masked_token_ids = []\n",
    "    word_index = 0\n",
    "\n",
    "    for token in tokenized_sentence:\n",
    "        if word_index < len(pos_tagged_sentence):\n",
    "            word, tag = pos_tagged_sentence[word_index]\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                masking = (tag == pos_tag)\n",
    "\n",
    "            if masking:\n",
    "                masked_sentence.append('[MASK]')\n",
    "                masked_token_ids.append(tokenizer.mask_token_id)\n",
    "\n",
    "            else:\n",
    "                masked_sentence.append(token)\n",
    "                masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                word_index += 1\n",
    "                \n",
    "        else:\n",
    "            masked_sentence.append(token)\n",
    "            masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "\n",
    "    return masked_sentence, masked_token_ids\n",
    "\n",
    "\n",
    "masked_sentence, masked_token_ids = mask_word_and_tokens(sentence, 'NN')\n",
    "\n",
    "print(f\"Masked Sentence: {masked_sentence}\")\n",
    "print(f\"Masked Token IDs: {masked_token_ids}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sentence: ['[MASK]', ',', '[MASK]', ',', '[MASK]', ',', '[MASK]', 'are', 'trying', 'to', 'learn', 'the', 'language', 'of', 'the', 'babies', '.', 'but', 'it', 'is', 'very', 'difficult', 'to', 'understand', 'the', 'babies', 'lan', '##ua', '##ge', '.', 'so', 'evan', 'decided', 'that', 'he', 'will', 'learn', 'the', 'nouns', 'first', '.', '[MASK]', 'will', 'learn', 'the', 'verbs', '.', '[MASK]', 'will', 'learn', 'the', 'adjective', '##s', '.', '[MASK]', 'will', 'learn', 'the', 'ad', '##ver', '##bs', '.']\n",
      "Masked Token IDs: [103, 1010, 103, 1010, 103, 1010, 103, 2024, 2667, 2000, 4553, 1996, 2653, 1997, 1996, 10834, 1012, 2021, 2009, 2003, 2200, 3697, 2000, 3305, 1996, 10834, 17595, 6692, 3351, 1012, 2061, 9340, 2787, 2008, 2002, 2097, 4553, 1996, 19211, 2034, 1012, 103, 2097, 4553, 1996, 16025, 1012, 103, 2097, 4553, 1996, 24931, 2015, 1012, 103, 2097, 4553, 1996, 4748, 6299, 5910, 1012]\n"
     ]
    }
   ],
   "source": [
    "##  code for masking the word and tokens for given pos tag but with percentage of masking \n",
    "\n",
    "# if i want to mask 50 percentage of  tag \"NN\" of the sentence then 50 percentage of the tag \"NN\" word should be masked and\n",
    "#  also the 50 percentage tokens of the tag \"NN\" word  should be masked \n",
    "\n",
    "def mask_word_and_tokens_with_percentage(sentence, pos_tag, percentage):\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tagged_sentence = nltk.pos_tag(words)\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "\n",
    "    target_words = [i for i, (word, tag) in enumerate(pos_tagged_sentence) if tag == pos_tag]\n",
    "\n",
    "    num_words_to_mask = max(1, int(len(target_words)* percentage / 100))\n",
    "\n",
    "    words_to_mask  = set(random.sample(target_words, num_words_to_mask))\n",
    "    \n",
    "    masked_sentence = []\n",
    "    masked_token_ids = []\n",
    "    word_index = 0\n",
    "\n",
    "    for token in tokenized_sentence:\n",
    "        if word_index < len(pos_tagged_sentence):\n",
    "            word, tag = pos_tagged_sentence[word_index]\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                masking = (word_index in words_to_mask)\n",
    "\n",
    "            if masking:\n",
    "                masked_sentence.append('[MASK]')\n",
    "                masked_token_ids.append(tokenizer.mask_token_id)\n",
    "\n",
    "            else:\n",
    "                masked_sentence.append(token)\n",
    "                masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                word_index += 1\n",
    "                \n",
    "        else:\n",
    "            masked_sentence.append(token)\n",
    "            masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "    \n",
    "    return masked_sentence, masked_token_ids\n",
    "\n",
    "masked_sentence, masked_token_ids = mask_word_and_tokens_with_percentage(sentence, 'NNP', 100)\n",
    "\n",
    "print(f\"Masked Sentence: {masked_sentence}\")\n",
    "print(f\"Masked Token IDs: {masked_token_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sentence: ['[MASK]', ',', '[MASK]', ',', '[MASK]', ',', '[MASK]', 'are', 'trying', 'to', 'learn', 'the', 'language', 'of', 'the', 'babies', '.', 'but', 'it', 'is', 'very', 'difficult', 'to', 'understand', 'the', 'babies', 'lan', '##ua', '##ge', '.', 'so', 'evan', 'decided', 'that', 'he', 'will', 'learn', 'the', 'nouns', 'first', '.', '[MASK]', 'will', 'learn', 'the', 'verbs', '.', '[MASK]', 'will', 'learn', 'the', 'adjective', '##s', '.', '[MASK]', 'will', 'learn', 'the', 'ad', '##ver', '##bs', '.']\n",
      "Masked Token IDs: [103, 1010, 103, 1010, 103, 1010, 103, 2024, 2667, 2000, 4553, 1996, 2653, 1997, 1996, 10834, 1012, 2021, 2009, 2003, 2200, 3697, 2000, 3305, 1996, 10834, 17595, 6692, 3351, 1012, 2061, 9340, 2787, 2008, 2002, 2097, 4553, 1996, 19211, 2034, 1012, 103, 2097, 4553, 1996, 16025, 1012, 103, 2097, 4553, 1996, 24931, 2015, 1012, 103, 2097, 4553, 1996, 4748, 6299, 5910, 1012]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"\n",
    " Evan, Kevin, Tim, Dylan are trying to learn the language of the babies. But it is very difficult to understand the babies lanuage. \n",
    "So evan decided that he will learn the nouns first. Kevin will learn the verbs. Tim will learn the adjectives. Dylan will learn the adverbs.\n",
    "\"\"\"\n",
    "\n",
    "masked_sentence, masked_token_ids = mask_word_and_tokens_with_percentage(sentence, 'NNP', 100)\n",
    "\n",
    "print(f\"Masked Sentence: {masked_sentence}\")\n",
    "print(f\"Masked Token IDs: {masked_token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sentence: ['evan', ',', '[MASK]', ',', '[MASK]', ',', 'dylan', 'are', 'trying', 'to', 'learn', 'the', '[MASK]', 'of', 'the', 'babies', '.', 'but', 'it', 'is', 'very', 'difficult', 'to', 'understand', 'the', 'babies', 'lan', '##ua', '##ge', '.', 'so', 'evan', 'decided', 'that', 'he', 'will', 'learn', 'the', 'nouns', 'first', '.', 'kevin', 'will', 'learn', 'the', '[MASK]', '.', '[MASK]', 'will', 'learn', 'the', 'adjective', '##s', '.', '[MASK]', 'will', 'learn', 'the', '[MASK]', '[MASK]', '[MASK]', '.']\n",
      "Masked Token IDs: [9340, 1010, 103, 1010, 103, 1010, 7758, 2024, 2667, 2000, 4553, 1996, 103, 1997, 1996, 10834, 1012, 2021, 2009, 2003, 2200, 3697, 2000, 3305, 1996, 10834, 17595, 6692, 3351, 1012, 2061, 9340, 2787, 2008, 2002, 2097, 4553, 1996, 19211, 2034, 1012, 4901, 2097, 4553, 1996, 103, 1012, 103, 2097, 4553, 1996, 24931, 2015, 1012, 103, 2097, 4553, 1996, 103, 103, 103, 1012]\n"
     ]
    }
   ],
   "source": [
    "# 103 is the mask token id and I think there  is difference between NN adn NNP \n",
    "# next setp is instead of only one \"NN\" we can send a list of pos tags and mask the words and tokens of the given pos tags\n",
    "\n",
    "\n",
    "def mask_word_and_tokens_with_percentage(sentence, pos_tags, percentage):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tagged_sentence = nltk.pos_tag(words)\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Find the indices of words matching the POS tags\n",
    "    target_words = [i for i, (word, tag) in enumerate(pos_tagged_sentence) if tag in pos_tags]\n",
    "\n",
    "    # Determine the number of words to mask\n",
    "    num_words_to_mask = max(1, int(len(target_words) * percentage / 100))\n",
    "\n",
    "    # Randomly select words to mask\n",
    "    words_to_mask = set(random.sample(target_words, num_words_to_mask))\n",
    "    \n",
    "    masked_sentence = []\n",
    "    masked_token_ids = []\n",
    "    word_index = 0\n",
    "\n",
    "    # Masking process\n",
    "    for token in tokenized_sentence:\n",
    "        if word_index < len(pos_tagged_sentence):\n",
    "            word, tag = pos_tagged_sentence[word_index]\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                masking = (word_index in words_to_mask)\n",
    "\n",
    "            if masking:\n",
    "                masked_sentence.append('[MASK]')\n",
    "                masked_token_ids.append(tokenizer.mask_token_id)\n",
    "            else:\n",
    "                masked_sentence.append(token)\n",
    "                masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                word_index += 1\n",
    "        else:\n",
    "            masked_sentence.append(token)\n",
    "            masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "    \n",
    "    return masked_sentence, masked_token_ids\n",
    "\n",
    "percentage = 70\n",
    "pos_tags = ['NN', 'NNP']\n",
    "\n",
    "masked_sentence, masked_token_ids = mask_word_and_tokens_with_percentage(sentence, pos_tags, percentage)\n",
    "\n",
    "print(f\"Masked Sentence: {masked_sentence}\")\n",
    "print(f\"Masked Token IDs: {masked_token_ids}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sentence: ['evan', ',', 'kevin', ',', 'tim', ',', 'dylan', 'are', 'trying', 'to', 'learn', 'the', '[MASK]', 'of', 'the', 'babies', '.', 'but', 'it', 'is', 'very', 'difficult', 'to', 'understand', 'the', 'babies', 'lan', '##ua', '##ge', '.', 'so', 'evan', 'decided', 'that', '[MASK]', 'will', 'learn', 'the', 'nouns', 'first', '.', 'kevin', 'will', 'learn', 'the', 'verbs', '.', 'tim', 'will', 'learn', 'the', 'adjective', '##s', '.', 'dylan', 'will', 'learn', 'the', '[MASK]', '[MASK]', '[MASK]', '.']\n",
      "Masked Token IDs: [9340, 1010, 4901, 1010, 5199, 1010, 7758, 2024, 2667, 2000, 4553, 1996, 103, 1997, 1996, 10834, 1012, 2021, 2009, 2003, 2200, 3697, 2000, 3305, 1996, 10834, 17595, 6692, 3351, 1012, 2061, 9340, 2787, 2008, 103, 2097, 4553, 1996, 19211, 2034, 1012, 4901, 2097, 4553, 1996, 16025, 1012, 5199, 2097, 4553, 1996, 24931, 2015, 1012, 7758, 2097, 4553, 1996, 103, 103, 103, 1012]\n"
     ]
    }
   ],
   "source": [
    "# next step is to start word masking weights toward interjections, nouns and personal pronouns (UH, NN, PRP)\n",
    "# this includes a binary noun/not-noun mask first and select % of nouns desired(50%) to achieve total mask percentage desired.\n",
    "\n",
    "def mask_word_and_tokens_with_percentage(sentence, pos_tag_weights, total_mask_percentage):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tagged_sentence = nltk.pos_tag(words)\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Create a list of words with their POS tag and their corresponding weights\n",
    "    target_words = []\n",
    "    for i, (word, tag) in enumerate(pos_tagged_sentence):\n",
    "        if tag in pos_tag_weights:\n",
    "            target_words.append((i, tag, pos_tag_weights[tag]))  # Append index, tag, and weight\n",
    "    \n",
    "    # Calculate total number of words to mask based on the overall percentage\n",
    "    total_words_to_mask = max(1, int(len(target_words) * total_mask_percentage / 100))\n",
    "\n",
    "    # Sort target words by their weights (higher weights first)\n",
    "    target_words_sorted = sorted(target_words, key=lambda x: -x[2])\n",
    "\n",
    "    # Randomly select words to mask, weighted by the given POS tags\n",
    "    selected_words = random.sample(target_words_sorted, total_words_to_mask)\n",
    "    words_to_mask = set(i for i, tag, weight in selected_words)\n",
    "    \n",
    "    masked_sentence = []\n",
    "    masked_token_ids = []\n",
    "    word_index = 0\n",
    "\n",
    "    # Masking process\n",
    "    for token in tokenized_sentence:\n",
    "        if word_index < len(pos_tagged_sentence):\n",
    "            word, tag = pos_tagged_sentence[word_index]\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                masking = (word_index in words_to_mask)\n",
    "\n",
    "            if masking:\n",
    "                masked_sentence.append('[MASK]')\n",
    "                masked_token_ids.append(tokenizer.mask_token_id)\n",
    "            else:\n",
    "                masked_sentence.append(token)\n",
    "                masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                word_index += 1\n",
    "        else:\n",
    "            masked_sentence.append(token)\n",
    "            masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "    \n",
    "    return masked_sentence, masked_token_ids\n",
    "\n",
    "# Define POS tags with their corresponding weights (higher weight = more likely to mask)\n",
    "pos_tag_weights = {\n",
    "    'UH': 0.4,  # Interjections\n",
    "    'NN': 0.3,  # Nouns\n",
    "    'PRP': 0.3  # Personal pronouns\n",
    "}\n",
    "\n",
    "# Mask 50% of words in total\n",
    "total_mask_percentage = 50\n",
    "\n",
    "masked_sentence, masked_token_ids = mask_word_and_tokens_with_percentage(sentence, pos_tag_weights, total_mask_percentage)\n",
    "\n",
    "print(f\"Masked Sentence: {masked_sentence}\")\n",
    "print(f\"Masked Token IDs: {masked_token_ids}\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked Sentence: ['tim', 'and', 'jung', '##yu', '##n', '[MASK]', '[MASK]', 'to', '[MASK]', 'how', 'to', '[MASK]', '[MASK]', 'of', 'babies', 'and', 'understand', 'their', '[MASK]', '.', 'they', 'are', 'finding', 'it', 'extremely', 'difficult', 'to', 'grasp', 'the', 'understanding', '##s', 'of', 'photon', '[MASK]', '[MASK]', '.', 'the', '[MASK]', '[MASK]', '[MASK]', 'to', 'help', 'them', 'understand', 'the', '[MASK]', \"'\", 'language', '.', 'they', '[MASK]', 'trying', 'to', '[MASK]', 'the', '[MASK]', 'first', '.', 'tim', 'will', '[MASK]', 'the', '[MASK]', '.', 'jung', '##yu', '##n', 'will', 'learn', 'the', '[MASK]', '[MASK]', '.', 'the', 'aliens', 'will', 'learn', 'the', 'ad', '##ver', '##bs', '.', '[MASK]', ',', 'tim', 'said', ',', 'we', 'can', 'now', 'be', '[MASK]', 'with', 'the', '[MASK]', 'and', '[MASK]', 'alike', '.']\n",
      "Masked Token IDs: [5199, 1998, 11810, 10513, 2078, 103, 103, 2000, 103, 2129, 2000, 103, 103, 1997, 10834, 1998, 3305, 2037, 103, 1012, 2027, 2024, 4531, 2009, 5186, 3697, 2000, 10616, 1996, 4824, 2015, 1997, 26383, 103, 103, 1012, 1996, 103, 103, 103, 2000, 2393, 2068, 3305, 1996, 103, 1005, 2653, 1012, 2027, 103, 2667, 2000, 103, 1996, 103, 2034, 1012, 5199, 2097, 103, 1996, 103, 1012, 11810, 10513, 2078, 2097, 4553, 1996, 103, 103, 1012, 1996, 12114, 2097, 4553, 1996, 4748, 6299, 5910, 1012, 103, 1010, 5199, 2056, 1010, 2057, 2064, 2085, 2022, 103, 2007, 1996, 103, 1998, 103, 11455, 1012]\n"
     ]
    }
   ],
   "source": [
    "#word masking weights toward the given list of Pos tags \n",
    "# first we have to do binary noun/not noun mask first and select % of nouns desired(50%) to achieve total mask percentage desired.\n",
    "\n",
    "\n",
    "sentence = \"\"\"\n",
    "    Tim and Jungyun are trying to learn how to take care of babies and understand their language. They are finding it extremely difficult to grasp the understandings of photon microwave signals. The aliens are coming\n",
    "    to help them understand the babies' language. They are trying to learn the nouns first. Tim will learn the verbs. Jungyun will learn the adjectives. The aliens will learn the adverbs. Ah, Tim said, we\n",
    "    can now be friends with the babies and aliens alike.\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def mask_word_and_tokens_with_weights_and_percentage(sentence, pos_tag_weights, total_mask_percentage):\n",
    "    # Tokenize the sentence\n",
    "    words = word_tokenize(sentence)\n",
    "    pos_tagged_sentence = nltk.pos_tag(words)\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Create a list of words with their POS tag and their corresponding weights\n",
    "    target_words = []\n",
    "    for i, (word, tag) in enumerate(pos_tagged_sentence):\n",
    "        if tag in pos_tag_weights:\n",
    "            target_words.append((i, tag, pos_tag_weights[tag]))  # Append index, tag, and weight\n",
    "    \n",
    "    # Calculate total number of words to mask based on the overall percentage\n",
    "    total_words_to_mask = max(1, int(len(target_words) * total_mask_percentage / 100))\n",
    "\n",
    "    # Sort target words by their weights (higher weights first)\n",
    "    target_words_sorted = sorted(target_words, key=lambda x: -x[2])\n",
    "\n",
    "    # Randomly select words to mask, weighted by the given POS tags\n",
    "    selected_words = random.sample(target_words_sorted, total_words_to_mask)\n",
    "    words_to_mask = set(i for i, tag, weight in selected_words)\n",
    "    \n",
    "    masked_sentence = []\n",
    "    masked_token_ids = []\n",
    "    word_index = 0\n",
    "\n",
    "    # Masking process\n",
    "    for token in tokenized_sentence:\n",
    "        if word_index < len(pos_tagged_sentence):\n",
    "            word, tag = pos_tagged_sentence[word_index]\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                masking = (word_index in words_to_mask)\n",
    "\n",
    "            if masking:\n",
    "                masked_sentence.append('[MASK]')\n",
    "                masked_token_ids.append(tokenizer.mask_token_id)\n",
    "            else:\n",
    "                masked_sentence.append(token)\n",
    "                masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "\n",
    "            if not token.startswith('##'):\n",
    "                word_index += 1\n",
    "        else:\n",
    "            masked_sentence.append(token)\n",
    "            masked_token_ids.append(tokenizer.convert_tokens_to_ids(token))\n",
    "    \n",
    "    return masked_sentence, masked_token_ids\n",
    "\n",
    "# create a function to create the pos tag weights\n",
    "\n",
    "def create_pos_tag_weights(pos_tags, weights):\n",
    "    pos_tag_weights = {}\n",
    "    for tag, weight in zip(pos_tags, weights):\n",
    "        pos_tag_weights[tag] = weight\n",
    "    return pos_tag_weights\n",
    "\n",
    "# function to create weights based on percentage for each pos tag\n",
    "def create_weights_from_percentage(pos_tags1, percentage1, pos_tags2, percentage2):\n",
    "    weights = []\n",
    "    \n",
    "    # Convert percentages to a fraction of 1\n",
    "    total_percentage = percentage1 + percentage2\n",
    "    percentage1_fraction = percentage1 / total_percentage\n",
    "    percentage2_fraction = percentage2 / total_percentage\n",
    "\n",
    "    # Create random weights for pos_tags1 and normalize them to sum to percentage1_fraction\n",
    "    num_pos_tags1 = len(pos_tags1)\n",
    "    pos_tags1_weights = [random.random() for _ in range(num_pos_tags1)]\n",
    "    sum_pos_tags1_weights = sum(pos_tags1_weights)\n",
    "    normalized_pos_tags1_weights = [(w / sum_pos_tags1_weights) * percentage1_fraction for w in pos_tags1_weights]\n",
    "    \n",
    "    # Append these normalized weights to the weights list\n",
    "    weights.extend(normalized_pos_tags1_weights)\n",
    "    \n",
    "    # Create random weights for pos_tags2 and normalize them to sum to percentage2_fraction\n",
    "    num_pos_tags2 = len(pos_tags2)\n",
    "    pos_tags2_weights = [random.random() for _ in range(num_pos_tags2)]\n",
    "    sum_pos_tags2_weights = sum(pos_tags2_weights)\n",
    "    normalized_pos_tags2_weights = [(w / sum_pos_tags2_weights) * percentage2_fraction for w in pos_tags2_weights]\n",
    "    \n",
    "    # Append these normalized weights to the weights list\n",
    "    weights.extend(normalized_pos_tags2_weights)\n",
    "    weights = [round(w, 2) for w in weights]\n",
    "    \n",
    "    return weights\n",
    "\n",
    "\n",
    "# Define POS tags with their corresponding weights (higher weight = more likely to mask)\n",
    "pos_tags = ['NN','NNP','NNPS','NNS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "weights = create_weights_from_percentage(pos_tags[:5], 70, pos_tags[5:], 30)\n",
    "pos_tag_weights = create_pos_tag_weights(pos_tags, weights)\n",
    "\n",
    "# Mask 50% of words in total and select % of nouns desired(50%) to achieve total mask percentage desired.\n",
    "total_mask_percentage = 50\n",
    "masked_sentence, masked_token_ids = mask_word_and_tokens_with_weights_and_percentage(sentence, pos_tag_weights, total_mask_percentage)\n",
    "\n",
    "print(f\"Masked Sentence: {masked_sentence}\")\n",
    "print(f\"Masked Token IDs: {masked_token_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training bert model to predict the masked words with the given sentence based oon the pos tags and weights \n",
    "# \n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babylmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
